Training iter #1500:   Batch Loss = 3.948874, Accuracy = 0.13333334028720856
PERFORMANCE ON TEST SET: Batch Loss = 3.062919855117798, Accuracy = 0.20563285052776337
Training iter #30000:   Batch Loss = 1.300843, Accuracy = 0.6966666579246521
PERFORMANCE ON TEST SET: Batch Loss = 1.4882047176361084, Accuracy = 0.5866983532905579
Training iter #60000:   Batch Loss = 1.191107, Accuracy = 0.7606666684150696
PERFORMANCE ON TEST SET: Batch Loss = 1.2652212381362915, Accuracy = 0.7319307923316956
Training iter #90000:   Batch Loss = 1.005859, Accuracy = 0.8193333148956299
PERFORMANCE ON TEST SET: Batch Loss = 1.145902395248413, Accuracy = 0.7807940244674683
Training iter #120000:   Batch Loss = 0.860981, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 1.2990336418151855, Accuracy = 0.7662029266357422
Training iter #150000:   Batch Loss = 0.764160, Accuracy = 0.9166666865348816
PERFORMANCE ON TEST SET: Batch Loss = 1.099226951599121, Accuracy = 0.826603353023529
Training iter #180000:   Batch Loss = 0.907124, Accuracy = 0.8546666502952576
PERFORMANCE ON TEST SET: Batch Loss = 1.2112610340118408, Accuracy = 0.8249067068099976
Training iter #210000:   Batch Loss = 0.836193, Accuracy = 0.8579999804496765
PERFORMANCE ON TEST SET: Batch Loss = 0.955629825592041, Accuracy = 0.861214816570282
Training iter #240000:   Batch Loss = 0.640759, Accuracy = 0.9633333086967468
PERFORMANCE ON TEST SET: Batch Loss = 0.906940221786499, Accuracy = 0.8903970122337341
Training iter #270000:   Batch Loss = 0.600320, Accuracy = 0.968666672706604
PERFORMANCE ON TEST SET: Batch Loss = 0.8929151296615601, Accuracy = 0.8931116461753845
Training iter #300000:   Batch Loss = 0.571598, Accuracy = 0.9826666712760925
PERFORMANCE ON TEST SET: Batch Loss = 0.928421139717102, Accuracy = 0.8819137811660767
Training iter #330000:   Batch Loss = 0.651419, Accuracy = 0.9559999704360962
PERFORMANCE ON TEST SET: Batch Loss = 0.9985215663909912, Accuracy = 0.8618934750556946
Training iter #360000:   Batch Loss = 0.621410, Accuracy = 0.9466666579246521
PERFORMANCE ON TEST SET: Batch Loss = 0.9246671795845032, Accuracy = 0.8758059144020081
Training iter #390000:   Batch Loss = 0.675342, Accuracy = 0.9246666431427002
PERFORMANCE ON TEST SET: Batch Loss = 0.9138436317443848, Accuracy = 0.861214816570282
Training iter #420000:   Batch Loss = 0.609039, Accuracy = 0.9240000247955322
PERFORMANCE ON TEST SET: Batch Loss = 0.9277873039245605, Accuracy = 0.8642687201499939
Training iter #450000:   Batch Loss = 0.575128, Accuracy = 0.9480000138282776
PERFORMANCE ON TEST SET: Batch Loss = 0.8765321969985962, Accuracy = 0.8890396952629089
Training iter #480000:   Batch Loss = 0.581805, Accuracy = 0.9326666593551636
PERFORMANCE ON TEST SET: Batch Loss = 0.8065533638000488, Accuracy = 0.9005768299102783
Training iter #510000:   Batch Loss = 0.518341, Accuracy = 0.9760000109672546
PERFORMANCE ON TEST SET: Batch Loss = 0.8153346180915833, Accuracy = 0.898201584815979
Training iter #540000:   Batch Loss = 0.567165, Accuracy = 0.9419999718666077
PERFORMANCE ON TEST SET: Batch Loss = 0.8326074481010437, Accuracy = 0.8965049386024475
Training iter #570000:   Batch Loss = 0.575105, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.8568381071090698, Accuracy = 0.8870037198066711
Training iter #600000:   Batch Loss = 0.549816, Accuracy = 0.9340000152587891
PERFORMANCE ON TEST SET: Batch Loss = 0.8134384155273438, Accuracy = 0.8900576829910278
Training iter #630000:   Batch Loss = 0.465868, Accuracy = 0.9733333587646484
PERFORMANCE ON TEST SET: Batch Loss = 0.9140022397041321, Accuracy = 0.8795385360717773
Training iter #660000:   Batch Loss = 0.456817, Accuracy = 0.984000027179718
PERFORMANCE ON TEST SET: Batch Loss = 0.7804354429244995, Accuracy = 0.8954869508743286
Training iter #690000:   Batch Loss = 0.416861, Accuracy = 0.9986666440963745
PERFORMANCE ON TEST SET: Batch Loss = 0.7833933234214783, Accuracy = 0.8920936584472656
Training iter #720000:   Batch Loss = 0.498195, Accuracy = 0.9520000219345093
PERFORMANCE ON TEST SET: Batch Loss = 0.8003355264663696, Accuracy = 0.8900576829910278
Training iter #750000:   Batch Loss = 0.521753, Accuracy = 0.9353333115577698
PERFORMANCE ON TEST SET: Batch Loss = 0.7413976192474365, Accuracy = 0.8975229263305664
Training iter #780000:   Batch Loss = 0.425659, Accuracy = 0.9639999866485596
PERFORMANCE ON TEST SET: Batch Loss = 0.7905454635620117, Accuracy = 0.891414999961853
Training iter #810000:   Batch Loss = 0.464539, Accuracy = 0.9520000219345093
PERFORMANCE ON TEST SET: Batch Loss = 0.7772055268287659, Accuracy = 0.8876823782920837
Training iter #840000:   Batch Loss = 0.458769, Accuracy = 0.9453333616256714
PERFORMANCE ON TEST SET: Batch Loss = 0.8225877285003662, Accuracy = 0.8798778653144836
Training iter #870000:   Batch Loss = 0.401934, Accuracy = 0.9779999852180481
PERFORMANCE ON TEST SET: Batch Loss = 0.866254448890686, Accuracy = 0.8815745115280151
Training iter #900000:   Batch Loss = 0.388762, Accuracy = 0.9806666374206543
PERFORMANCE ON TEST SET: Batch Loss = 0.8430436253547668, Accuracy = 0.8771632313728333
Training iter #930000:   Batch Loss = 0.467655, Accuracy = 0.9259999990463257
PERFORMANCE ON TEST SET: Batch Loss = 0.704870879650116, Accuracy = 0.9107567071914673
Training iter #960000:   Batch Loss = 0.481189, Accuracy = 0.921999990940094
PERFORMANCE ON TEST SET: Batch Loss = 0.7424818277359009, Accuracy = 0.8866643905639648
Training iter #990000:   Batch Loss = 0.546949, Accuracy = 0.909333348274231
PERFORMANCE ON TEST SET: Batch Loss = 1.0894891023635864, Accuracy = 0.7919918298721313
Training iter #1020000:   Batch Loss = 0.532725, Accuracy = 0.9433333277702332
PERFORMANCE ON TEST SET: Batch Loss = 0.7003955841064453, Accuracy = 0.8771632313728333
Training iter #1050000:   Batch Loss = 0.421475, Accuracy = 0.9786666631698608
PERFORMANCE ON TEST SET: Batch Loss = 0.6862202882766724, Accuracy = 0.8907363414764404
Training iter #1080000:   Batch Loss = 0.420236, Accuracy = 0.9739999771118164
PERFORMANCE ON TEST SET: Batch Loss = 0.7144688367843628, Accuracy = 0.8870037198066711
Training iter #1110000:   Batch Loss = 0.452686, Accuracy = 0.9346666932106018
PERFORMANCE ON TEST SET: Batch Loss = 0.6831488013267517, Accuracy = 0.8971835970878601
Training iter #1140000:   Batch Loss = 0.444131, Accuracy = 0.9353333115577698
PERFORMANCE ON TEST SET: Batch Loss = 0.688616156578064, Accuracy = 0.8968442678451538
Training iter #1170000:   Batch Loss = 0.400558, Accuracy = 0.9486666917800903
PERFORMANCE ON TEST SET: Batch Loss = 0.6862390041351318, Accuracy = 0.8988802433013916
Training iter #1200000:   Batch Loss = 0.383193, Accuracy = 0.9546666741371155
PERFORMANCE ON TEST SET: Batch Loss = 0.6846458911895752, Accuracy = 0.8985409140586853
Training iter #1230000:   Batch Loss = 0.371628, Accuracy = 0.9553333520889282
PERFORMANCE ON TEST SET: Batch Loss = 0.7168514728546143, Accuracy = 0.8931116461753845
Training iter #1260000:   Batch Loss = 0.345853, Accuracy = 0.9766666889190674
PERFORMANCE ON TEST SET: Batch Loss = 0.7048674821853638, Accuracy = 0.8951476216316223
Training iter #1290000:   Batch Loss = 0.384374, Accuracy = 0.9493333101272583
PERFORMANCE ON TEST SET: Batch Loss = 0.7132728099822998, Accuracy = 0.8924329876899719
Training iter #1320000:   Batch Loss = 0.383681, Accuracy = 0.9433333277702332
PERFORMANCE ON TEST SET: Batch Loss = 0.688133716583252, Accuracy = 0.8924329876899719
Training iter #1350000:   Batch Loss = 0.467493, Accuracy = 0.9253333210945129
PERFORMANCE ON TEST SET: Batch Loss = 0.7087751626968384, Accuracy = 0.8703766465187073
Training iter #1380000:   Batch Loss = 0.416235, Accuracy = 0.9559999704360962
PERFORMANCE ON TEST SET: Batch Loss = 0.6310989260673523, Accuracy = 0.8920936584472656
Training iter #1410000:   Batch Loss = 0.467071, Accuracy = 0.9366666674613953
PERFORMANCE ON TEST SET: Batch Loss = 0.8187867999076843, Accuracy = 0.8568035364151001
Training iter #1440000:   Batch Loss = 0.413089, Accuracy = 0.9559999704360962
PERFORMANCE ON TEST SET: Batch Loss = 0.6449697017669678, Accuracy = 0.8931116461753845
Training iter #1470000:   Batch Loss = 0.407021, Accuracy = 0.9526666402816772

PERFORMANCE ON TEST SET: Batch Loss = 0.5786199569702148, Accuracy = 0.9148286581039429
Training iter #1500000:   Batch Loss = 0.413031, Accuracy = 0.9359999895095825
PERFORMANCE ON TEST SET: Batch Loss = 0.5879096984863281, Accuracy = 0.913810670375824
Training iter #1530000:   Batch Loss = 0.352553, Accuracy = 0.9566666483879089
PERFORMANCE ON TEST SET: Batch Loss = 0.6069459915161133, Accuracy = 0.9083814024925232
Training iter #1560000:   Batch Loss = 0.340831, Accuracy = 0.9580000042915344
PERFORMANCE ON TEST SET: Batch Loss = 0.5970805883407593, Accuracy = 0.913810670375824
Training iter #1590000:   Batch Loss = 0.352259, Accuracy = 0.9466666579246521
PERFORMANCE ON TEST SET: Batch Loss = 0.616621732711792, Accuracy = 0.8992195725440979
Training iter #1620000:   Batch Loss = 0.310026, Accuracy = 0.9786666631698608
PERFORMANCE ON TEST SET: Batch Loss = 0.6469941735267639, Accuracy = 0.8975229263305664
Training iter #1650000:   Batch Loss = 0.348754, Accuracy = 0.9346666932106018
PERFORMANCE ON TEST SET: Batch Loss = 0.65960294008255, Accuracy = 0.8995589017868042
Training iter #1680000:   Batch Loss = 0.360530, Accuracy = 0.9340000152587891
PERFORMANCE ON TEST SET: Batch Loss = 0.6702477335929871, Accuracy = 0.9012554883956909
Training iter #1710000:   Batch Loss = 0.356913, Accuracy = 0.9346666932106018
PERFORMANCE ON TEST SET: Batch Loss = 0.6591196656227112, Accuracy = 0.8992195725440979
Training iter #1740000:   Batch Loss = 0.303982, Accuracy = 0.9773333072662354
PERFORMANCE ON TEST SET: Batch Loss = 0.6248706579208374, Accuracy = 0.8941296339035034
Training iter #1770000:   Batch Loss = 0.315492, Accuracy = 0.9526666402816772
PERFORMANCE ON TEST SET: Batch Loss = 0.6825898289680481, Accuracy = 0.8900576829910278
Training iter #1800000:   Batch Loss = 0.291599, Accuracy = 0.984666645526886
PERFORMANCE ON TEST SET: Batch Loss = 0.6363216638565063, Accuracy = 0.8968442678451538
Training iter #1830000:   Batch Loss = 0.341221, Accuracy = 0.9539999961853027
PERFORMANCE ON TEST SET: Batch Loss = 0.5785644054412842, Accuracy = 0.9073634147644043
Training iter #1860000:   Batch Loss = 0.363568, Accuracy = 0.940666675567627
PERFORMANCE ON TEST SET: Batch Loss = 0.6445761322975159, Accuracy = 0.8937903046607971
Training iter #1890000:   Batch Loss = 0.310326, Accuracy = 0.9520000219345093
PERFORMANCE ON TEST SET: Batch Loss = 0.6376086473464966, Accuracy = 0.8944689631462097
Training iter #1920000:   Batch Loss = 0.285511, Accuracy = 0.9606666564941406
PERFORMANCE ON TEST SET: Batch Loss = 0.6329972743988037, Accuracy = 0.9022734761238098
Training iter #1950000:   Batch Loss = 0.294607, Accuracy = 0.9566666483879089
PERFORMANCE ON TEST SET: Batch Loss = 0.7293813228607178, Accuracy = 0.8903970122337341
Training iter #1980000:   Batch Loss = 0.792753, Accuracy = 0.7806666493415833
PERFORMANCE ON TEST SET: Batch Loss = 0.9718454480171204, Accuracy = 0.8089582920074463
Training iter #2010000:   Batch Loss = 0.390357, Accuracy = 0.9493333101272583
PERFORMANCE ON TEST SET: Batch Loss = 0.7628068327903748, Accuracy = 0.8289785981178284
Training iter #2040000:   Batch Loss = 0.443690, Accuracy = 0.8666666746139526
PERFORMANCE ON TEST SET: Batch Loss = 0.54832923412323, Accuracy = 0.8883610367774963
Training iter #2070000:   Batch Loss = 0.369296, Accuracy = 0.9266666769981384
PERFORMANCE ON TEST SET: Batch Loss = 0.5818323493003845, Accuracy = 0.8927723169326782
Training iter #2100000:   Batch Loss = 0.273590, Accuracy = 0.9893333315849304
PERFORMANCE ON TEST SET: Batch Loss = 0.6006695032119751, Accuracy = 0.8903970122337341
Training iter #2130000:   Batch Loss = 0.274792, Accuracy = 0.9860000014305115
PERFORMANCE ON TEST SET: Batch Loss = 0.5898439884185791, Accuracy = 0.8920936584472656
Training iter #2160000:   Batch Loss = 0.259621, Accuracy = 0.987333357334137
PERFORMANCE ON TEST SET: Batch Loss = 0.5876603126525879, Accuracy = 0.8934509754180908
Training iter #2190000:   Batch Loss = 0.279541, Accuracy = 0.9833333492279053
PERFORMANCE ON TEST SET: Batch Loss = 0.6032086610794067, Accuracy = 0.8920936584472656
Optimization Finished!
FINAL RESULT: Batch Loss = 0.6084891557693481, Accuracy = 0.8897183537483215







Testing Accuracy: 88.97183537483215%

Precision: 88.99611282611261%
Recall: 88.97183576518493%
f1_score: 88.91043032299801%

Confusion Matrix:
[[461   3  31   1   0   0]
 [ 32 413  26   0   0   0]
 [ 15   7 398   0   0   0]
 [  0  18   0 399  68   6]
 [  3   7   0 108 414   0]
 [  0   0   0   0   0 537]]

Confusion matrix (normalised to % of total test data):
[[ 15.64302731   0.10179844   1.0519172    0.03393281   0.           0.        ]
 [  1.08585     14.01425171   0.88225317   0.           0.           0.        ]
 [  0.5089922    0.2375297   13.50525951   0.           0.           0.        ]
 [  0.           0.61079061   0.          13.5391922    2.30743122
    0.20359688]
 [  0.10179844   0.2375297    0.           3.6647439   14.04818439   0.        ]
 [  0.           0.           0.           0.           0.          18.22192001]]
Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.

